# CandyMatch: A Novel Distillation Loss Function for Creating Small, Effective AI Tutors

## Abstract
The use of Large Language Models (LLMs) as AI tutors holds immense promise, but their size and cost make them difficult to scale. Knowledge distillation, the process of training a smaller "student" model to mimic a larger "teacher" model, offers a potential solution. This paper introduces CandyMatch, a novel distillation loss function designed specifically for creating AI tutors. CandyMatch goes beyond simple output matching, encouraging the student model to learn the underlying reasoning process of the teacher by incorporating measures of task relevance and entropy weighting. We demonstrate the effectiveness of CandyMatch by using it to distill a Mistral-7B teacher model into a 1B parameter student model for a multi-agent AI tutoring system. Our results show that the CandyMatch-distilled student model significantly outperforms a student model distilled with a traditional loss function on a range of educational tasks.

## Introduction
The vision of a personal AI tutor for every student is closer than ever, thanks to the power of Large Language Models (LLMs). These models can explain complex concepts, generate practice problems, and provide personalized feedback. However, the computational resources required to run these models make them inaccessible to many. Knowledge distillation offers a path towards democratizing AI education by creating smaller, more efficient models. The key challenge in knowledge distillation is designing a loss function that effectively transfers the "knowledge" from the teacher to the student. This is particularly important for educational applications, where it's not enough for the student to simply mimic the teacher's answers; it must also learn the underlying reasoning.

## Related Works
Knowledge distillation has been successfully applied in a variety of domains. The most common approach is to use a loss function based on the Kullback-Leibler (KL) divergence between the teacher's and student's output distributions. While effective, this approach does not explicitly encourage the student to learn the teacher's reasoning process. More recent work has explored using attention-based methods to encourage the student to focus on the same parts of the input as the teacher. CandyMatch builds on these ideas but introduces two key innovations: 1) a task relevance metric that helps the student focus on the most important parts of the educational task, and 2) an entropy weighting mechanism that encourages the student to be more confident in its correct answers. This combination results in a student model that is not only smaller and faster but also a more effective AI tutor.
